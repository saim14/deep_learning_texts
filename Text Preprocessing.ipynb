{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text Preprocessing.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNHHtwCJJv0nDB0JAKq59Ho"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YonSyIn3HG6Z"},"source":["<h1>Two example of load and preprocess text</h1>"]},{"cell_type":"markdown","metadata":{"id":"cCY5gXX-KTXZ"},"source":["# Import library\n"]},{"cell_type":"code","metadata":{"id":"sHDGCBZjGQfI","executionInfo":{"status":"ok","timestamp":1606655222047,"user_tz":-360,"elapsed":3894,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}}},"source":["import collections\n","import pathlib\n","import re\n","import string\n","\n","import tensorflow as tf\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras import losses\n","from tensorflow.keras import preprocessing\n","from tensorflow.keras import utils\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n","\n","import tensorflow_datasets as tfds"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"abywRzHZHRGL","executionInfo":{"status":"ok","timestamp":1606655250799,"user_tz":-360,"elapsed":4422,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"9f1195eb-bd01-4b14-8b7c-4b38449ad9fa"},"source":["!pip install tensorflow-text"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting tensorflow-text\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b2/2dbd90b93913afd07e6101b8b84327c401c394e60141c1e98590038060b3/tensorflow_text-2.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.6MB)\n","\u001b[K     |████████████████████████████████| 2.6MB 9.8MB/s \n","\u001b[?25hRequirement already satisfied: tensorflow<2.4,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text) (2.3.0)\n","Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (1.6.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (1.15.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (1.12.1)\n","Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (2.3.0)\n","Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (2.10.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (3.3.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (3.12.4)\n","Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (1.1.2)\n","Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (1.18.5)\n","Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (0.2.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (0.10.0)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (0.3.3)\n","Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (2.3.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (0.35.1)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (1.4.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (1.1.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text) (1.33.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow<2.4,>=2.3.0->tensorflow-text) (50.3.2)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (1.17.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (3.3.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (2.23.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (1.7.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (4.6)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (4.1.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (0.2.8)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (2.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (2020.11.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (3.0.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (1.3.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (0.4.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (3.4.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text) (3.1.0)\n","Installing collected packages: tensorflow-text\n","Successfully installed tensorflow-text-2.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"spMFKYj7HX-1","executionInfo":{"status":"ok","timestamp":1606655265906,"user_tz":-360,"elapsed":992,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}}},"source":["import tensorflow_text as tf_text"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MrlYG6e3HhH0"},"source":["# Example 1: Predict the tag for a Stack Overflow question"]},{"cell_type":"markdown","metadata":{"id":"9XN44PRmHv4T"},"source":["As a first example, you will download a dataset of programming questions from Stack Overflow. Each question (\"How do I sort a dictionary by value?\") is labeled with exactly one tag (Python, CSharp, JavaScript, or Java). Your task is to develop a model that predicts the tag for a question. This is an example of multi-class classification, an important and widely applicable kind of machine learning problem."]},{"cell_type":"markdown","metadata":{"id":"i_ZEBtJvHtch"},"source":["## Download and explore the dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wmqE_89WHce3","executionInfo":{"status":"ok","timestamp":1606655385826,"user_tz":-360,"elapsed":4700,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"a1cd087c-5991-4671-d150-5114225294d2"},"source":["data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n","dataset = utils.get_file(\n","    'stack_overflow_16k.tar.gz',\n","    data_url,\n","    untar=True,\n","    cache_dir='stack_overflow',\n","    cache_subdir='')\n","dataset_dir = pathlib.Path(dataset).parent"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\n","6053888/6053168 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uAR2XyMpH44h","executionInfo":{"status":"ok","timestamp":1606655413485,"user_tz":-360,"elapsed":940,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"c8b68f3d-9856-458f-c80b-9934386cd11a"},"source":["list(dataset_dir.iterdir())"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[PosixPath('/tmp/.keras/stack_overflow_16k.tar.gz.tar.gz'),\n"," PosixPath('/tmp/.keras/README.md'),\n"," PosixPath('/tmp/.keras/train'),\n"," PosixPath('/tmp/.keras/test')]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iHUFAQ5AIAjC","executionInfo":{"status":"ok","timestamp":1606655457542,"user_tz":-360,"elapsed":964,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"ff228f72-e60f-4cce-fdf3-d9d174f7d07d"},"source":["train_dir = dataset_dir/'train'\n","list(train_dir.iterdir())"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[PosixPath('/tmp/.keras/train/csharp'),\n"," PosixPath('/tmp/.keras/train/javascript'),\n"," PosixPath('/tmp/.keras/train/python'),\n"," PosixPath('/tmp/.keras/train/java')]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"Mgej-YEZITRN"},"source":["*The train/csharp, train/java, train/python and train/javascript directories contain many text files, each of which is a Stack Overflow question. Print a file and inspect the data.*"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vC4rFNkdILTa","executionInfo":{"status":"ok","timestamp":1606655517240,"user_tz":-360,"elapsed":1117,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"757f29d0-2a5e-4c71-d7fc-e7d952986055"},"source":["sample_file = train_dir/'python/1755.txt'\n","with open(sample_file) as f:\n","  print(f.read())"],"execution_count":7,"outputs":[{"output_type":"stream","text":["why does this blank program print true x=true.def stupid():.    x=false.stupid().print x\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RovDIp16Ifs1"},"source":["## Load the dataset"]},{"cell_type":"markdown","metadata":{"id":"02OEy38VInKx"},"source":["When running a machine learning experiment, it is a best practice to divide your dataset into three splits: train, validation, and test. The Stack Overflow dataset has already been divided into train and test, but it lacks a validation set. Create a validation set using an 80:20 split of the training data by using the validation_split argument below."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hbhlejmdIZ1h","executionInfo":{"status":"ok","timestamp":1606655595699,"user_tz":-360,"elapsed":6685,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"5335cf4a-40af-4f0f-9814-5b349fb47be1"},"source":["batch_size = 32\n","seed = 42\n","\n","raw_train_ds = preprocessing.text_dataset_from_directory(\n","    train_dir,\n","    batch_size=batch_size,\n","    validation_split=0.2,\n","    subset='training',\n","    seed=seed)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Found 8000 files belonging to 4 classes.\n","Using 6400 files for training.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4kkunDdFI1ZI"},"source":["*As you can see above, there are 8,000 examples in the training folder, of which you will use 80% (or 6,400) for training. As you will see in a moment, you can train a model by passing a tf.data.Dataset directly to model.fit. First, iterate over the dataset and print out a few examples, to get a feel for the data.*"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DLcKEyibIroy","executionInfo":{"status":"ok","timestamp":1606655648437,"user_tz":-360,"elapsed":1191,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"805b47dd-f82a-4d0f-cc3f-987e2220cfd7"},"source":["for text_batch, label_batch in raw_train_ds.take(1):\n","  for i in range(10):\n","    print(\"Question: \", text_batch.numpy()[i][:100], '...')\n","    print(\"Label:\", label_batch.numpy()[i])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Question:  b'\"my tester is going to the wrong constructor i am new to programming so if i ask a question that can' ...\n","Label: 1\n","Question:  b'\"blank code slow skin detection this code changes the color space to lab and using a threshold finds' ...\n","Label: 3\n","Question:  b'\"option and validation in blank i want to add a new option on my system where i want to add two text' ...\n","Label: 1\n","Question:  b'\"exception: dynamic sql generation for the updatecommand is not supported against a selectcommand th' ...\n","Label: 0\n","Question:  b'\"parameter with question mark and super in blank, i\\'ve come across a method that is formatted like t' ...\n","Label: 1\n","Question:  b'call two objects wsdl the first time i got a very strange wsdl. ..i would like to call the object (i' ...\n","Label: 0\n","Question:  b'how to correctly make the icon for systemtray in blank using icon sizes of any dimension for systemt' ...\n","Label: 0\n","Question:  b'\"is there a way to check a variable that exists in a different script than the original one? i\\'m try' ...\n","Label: 3\n","Question:  b'\"blank control flow i made a number which asks for 2 numbers with blank and responds with  the corre' ...\n","Label: 0\n","Question:  b'\"credentials cannot be used for ntlm authentication i am getting org.apache.commons.httpclient.auth.' ...\n","Label: 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YSdO_9tWI517","executionInfo":{"status":"ok","timestamp":1606655692073,"user_tz":-360,"elapsed":1080,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"a125a162-529c-4a7c-f9b7-daa883b31524"},"source":["for i, label in enumerate(raw_train_ds.class_names):\n","  print(\"Label\", i, \"corresponds to\", label)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Label 0 corresponds to csharp\n","Label 1 corresponds to java\n","Label 2 corresponds to javascript\n","Label 3 corresponds to python\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iZ2nD8ECJEhn","executionInfo":{"status":"ok","timestamp":1606655955096,"user_tz":-360,"elapsed":892,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"094166b8-8c9a-466a-e481-4a81bde7b429"},"source":["raw_val_ds = preprocessing.text_dataset_from_directory(\n","    train_dir,\n","    batch_size=batch_size,\n","    validation_split=0.2,\n","    subset='validation',\n","    seed=seed)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Found 8000 files belonging to 4 classes.\n","Using 1600 files for validation.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ebUrCbeIKEw3","executionInfo":{"status":"ok","timestamp":1606655969779,"user_tz":-360,"elapsed":959,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"09609e93-35f2-4d28-f812-cd69c867d801"},"source":["test_dir = dataset_dir/'test'\n","raw_test_ds = preprocessing.text_dataset_from_directory(\n","    test_dir, batch_size=batch_size)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Found 8000 files belonging to 4 classes.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0WJiAknlKQKD"},"source":["## Prepare the dataset for training"]},{"cell_type":"markdown","metadata":{"id":"UT3kkyVhKv3_"},"source":["Next, you will **standardize, tokenize,** and **vectorize** the data using the preprocessing.TextVectorization layer"]},{"cell_type":"code","metadata":{"id":"oNEt3FbIKIW1","executionInfo":{"status":"ok","timestamp":1606656278662,"user_tz":-360,"elapsed":1156,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}}},"source":["VOCAB_SIZE = 10000\n","\n","binary_vectorize_layer = TextVectorization(\n","    max_tokens=VOCAB_SIZE,\n","    output_mode='binary')"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D_EyJorILbSH"},"source":["For int mode, in addition to maximum vocabulary size, you need to set an explicit maximum sequence length, which will cause the layer to pad or truncate sequences to exactly sequence_length values."]},{"cell_type":"code","metadata":{"id":"aZ5uzaMmLTs1","executionInfo":{"status":"ok","timestamp":1606656335424,"user_tz":-360,"elapsed":1006,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}}},"source":["MAX_SEQUENCE_LENGTH = 250\n","\n","int_vectorize_layer = TextVectorization(\n","    max_tokens=VOCAB_SIZE,\n","    output_mode='int',\n","    output_sequence_length=MAX_SEQUENCE_LENGTH)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v22qs_nTLnn7"},"source":["**Next, you will call adapt to fit the state of the preprocessing layer to the dataset. This will cause the model to build an index of strings to integers.**"]},{"cell_type":"code","metadata":{"id":"hgawCaumLhnR","executionInfo":{"status":"ok","timestamp":1606656412651,"user_tz":-360,"elapsed":7863,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}}},"source":["# Make a text-only dataset (without labels), then call adapt\n","train_text = raw_train_ds.map(lambda text, labels: text)\n","binary_vectorize_layer.adapt(train_text)\n","int_vectorize_layer.adapt(train_text)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LWeqLx2lL3FO"},"source":["**See the result of using these layers to preprocess data:**"]},{"cell_type":"code","metadata":{"id":"zS68Yh_MLyzJ","executionInfo":{"status":"ok","timestamp":1606656456367,"user_tz":-360,"elapsed":1076,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}}},"source":["def binary_vectorize_text(text, label):\n","  text = tf.expand_dims(text, -1)\n","  return binary_vectorize_layer(text), label\n","  \n","def int_vectorize_text(text, label):\n","  text = tf.expand_dims(text, -1)\n","  return int_vectorize_layer(text), label"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ruwt800ZL_Hv","executionInfo":{"status":"ok","timestamp":1606656496149,"user_tz":-360,"elapsed":954,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"df47d225-3287-457a-ac63-7afae06b2186"},"source":["# Retrieve a batch (of 32 reviews and labels) from the dataset\n","text_batch, label_batch = next(iter(raw_train_ds))\n","first_question, first_label = text_batch[0], label_batch[0]\n","print(\"Question\", first_question)\n","print(\"Label\", first_label)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Question tf.Tensor(b'\"function expected error in blank for dynamically created check box when it is clicked i want to grab the attribute value.it is working in ie 8,9,10 but not working in ie 11,chrome shows function expected error..&lt;input type=checkbox checked=\\'checked\\' id=\\'symptomfailurecodeid\\' tabindex=\\'54\\' style=\\'cursor:pointer;\\' onclick=chkclickevt(this);  failurecodeid=\"\"1\"\" &gt;...function chkclickevt(obj) { .    alert(obj.attributes(\"\"failurecodeid\"\"));.}\"\\n', shape=(), dtype=string)\n","Label tf.Tensor(2, shape=(), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dv4yHKU9MI3b","executionInfo":{"status":"ok","timestamp":1606656551769,"user_tz":-360,"elapsed":1005,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"32c7ceb9-de77-4608-d219-08cfcaf7315a"},"source":["print(\"'binary' vectorized question:\", \n","      binary_vectorize_text(first_question, first_label)[0])"],"execution_count":18,"outputs":[{"output_type":"stream","text":["'binary' vectorized question: tf.Tensor([[1. 1. 1. ... 0. 0. 0.]], shape=(1, 10000), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yD9szDfzMWbX","executionInfo":{"status":"ok","timestamp":1606656562743,"user_tz":-360,"elapsed":911,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"5d34c667-9780-47a8-99bd-e1556bce0ce7"},"source":["print(\"'int' vectorized question:\",\n","      int_vectorize_text(first_question, first_label)[0])"],"execution_count":19,"outputs":[{"output_type":"stream","text":["'int' vectorized question: tf.Tensor(\n","[[  38  450   65    7   16   12  892  265  186  451   44   11    6  685\n","     3   46    4 2062    2  485    1    6  158    7  479    1   26   20\n","   158    7  479    1  502   38  450    1 1767 1763    1    1    1    1\n","     1    1    1    1    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0]], shape=(1, 250), dtype=int64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wB5Oj7VIMjJ5"},"source":["**You can lookup the token (string) that each integer corresponds to by calling .get_vocabulary() on the layer.**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-bOX5Y5sMZGq","executionInfo":{"status":"ok","timestamp":1606656625560,"user_tz":-360,"elapsed":1132,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"2eb58f7a-45b8-4a0f-e019-ef81bb27591b"},"source":["print(\"1289 ---> \", int_vectorize_layer.get_vocabulary()[1289])\n","print(\"313 ---> \", int_vectorize_layer.get_vocabulary()[313])\n","print(\"Vocabulary size: {}\".format(len(int_vectorize_layer.get_vocabulary())))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["1289 --->  roman\n","313 --->  source\n","Vocabulary size: 10000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iYDXvcPHMu7s"},"source":["**You are nearly ready to train your model. As a final preprocessing step, you will apply the TextVectorization layers you created earlier to the train, validation, and test dataset.**"]},{"cell_type":"code","metadata":{"id":"5c5560n8MoZf","executionInfo":{"status":"ok","timestamp":1606656677316,"user_tz":-360,"elapsed":1024,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}}},"source":["binary_train_ds = raw_train_ds.map(binary_vectorize_text)\n","binary_val_ds = raw_val_ds.map(binary_vectorize_text)\n","binary_test_ds = raw_test_ds.map(binary_vectorize_text)\n","\n","int_train_ds = raw_train_ds.map(int_vectorize_text)\n","int_val_ds = raw_val_ds.map(int_vectorize_text)\n","int_test_ds = raw_test_ds.map(int_vectorize_text)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wz85aKyDM4aE"},"source":["## Configure the dataset for performance"]},{"cell_type":"code","metadata":{"id":"tM7YYzRLM1Eo","executionInfo":{"status":"ok","timestamp":1606656713639,"user_tz":-360,"elapsed":869,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}}},"source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","def configure_dataset(dataset):\n","  return dataset.cache().prefetch(buffer_size=AUTOTUNE)\n","  \n","binary_train_ds = configure_dataset(binary_train_ds)\n","binary_val_ds = configure_dataset(binary_val_ds)\n","binary_test_ds = configure_dataset(binary_test_ds)\n","\n","int_train_ds = configure_dataset(int_train_ds)\n","int_val_ds = configure_dataset(int_val_ds)\n","int_test_ds = configure_dataset(int_test_ds)"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PqRF355YNAxe"},"source":["## Train the model"]},{"cell_type":"markdown","metadata":{"id":"5QCOhWF6NTr-"},"source":["**It's time to create our neural network. For the binary vectorized data, train a simple bag-of-words linear model:**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aOLYgA_zM9-c","executionInfo":{"status":"ok","timestamp":1606656778199,"user_tz":-360,"elapsed":9768,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"b4d300e2-4dff-4eca-d4e5-f966991b1703"},"source":["binary_model = tf.keras.Sequential([layers.Dense(4)])\n","binary_model.compile(\n","    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer='adam',\n","    metrics=['accuracy'])\n","history = binary_model.fit(\n","    binary_train_ds, validation_data=binary_val_ds, epochs=10)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","200/200 [==============================] - 3s 13ms/step - loss: 1.1192 - accuracy: 0.6514 - val_loss: 0.9129 - val_accuracy: 0.7750\n","Epoch 2/10\n","200/200 [==============================] - 1s 3ms/step - loss: 0.7776 - accuracy: 0.8205 - val_loss: 0.7489 - val_accuracy: 0.7994\n","Epoch 3/10\n","200/200 [==============================] - 1s 3ms/step - loss: 0.6265 - accuracy: 0.8633 - val_loss: 0.6634 - val_accuracy: 0.8112\n","Epoch 4/10\n","200/200 [==============================] - 1s 3ms/step - loss: 0.5334 - accuracy: 0.8892 - val_loss: 0.6101 - val_accuracy: 0.8256\n","Epoch 5/10\n","200/200 [==============================] - 1s 3ms/step - loss: 0.4675 - accuracy: 0.9056 - val_loss: 0.5735 - val_accuracy: 0.8325\n","Epoch 6/10\n","200/200 [==============================] - 1s 3ms/step - loss: 0.4173 - accuracy: 0.9172 - val_loss: 0.5469 - val_accuracy: 0.8363\n","Epoch 7/10\n","200/200 [==============================] - 0s 2ms/step - loss: 0.3772 - accuracy: 0.9287 - val_loss: 0.5268 - val_accuracy: 0.8406\n","Epoch 8/10\n","200/200 [==============================] - 1s 3ms/step - loss: 0.3440 - accuracy: 0.9367 - val_loss: 0.5113 - val_accuracy: 0.8419\n","Epoch 9/10\n","200/200 [==============================] - 1s 3ms/step - loss: 0.3158 - accuracy: 0.9420 - val_loss: 0.4991 - val_accuracy: 0.8413\n","Epoch 10/10\n","200/200 [==============================] - 1s 3ms/step - loss: 0.2915 - accuracy: 0.9488 - val_loss: 0.4893 - val_accuracy: 0.8431\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u9QmwdWWNXu6"},"source":["**Next, you will use the int vectorized layer to build a 1D ConvNet.**"]},{"cell_type":"code","metadata":{"id":"tvCaVV6PNLi3","executionInfo":{"status":"ok","timestamp":1606656843456,"user_tz":-360,"elapsed":954,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}}},"source":["def create_model(vocab_size, num_labels):\n","  model = tf.keras.Sequential([\n","      layers.Embedding(vocab_size, 64, mask_zero=True),\n","      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n","      layers.GlobalMaxPooling1D(),\n","      layers.Dense(num_labels)\n","  ])\n","  return model"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yGeLT5nINdpw","executionInfo":{"status":"ok","timestamp":1606656885969,"user_tz":-360,"elapsed":27473,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"34f5a01e-5253-4964-d3f6-47b4c8d0a165"},"source":["# vocab_size is VOCAB_SIZE + 1 since 0 is used additionally for padding.\n","int_model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=4)\n","int_model.compile(\n","    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer='adam',\n","    metrics=['accuracy'])\n","history = int_model.fit(int_train_ds, validation_data=int_val_ds, epochs=10)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","200/200 [==============================] - 4s 19ms/step - loss: 1.1383 - accuracy: 0.5233 - val_loss: 0.7411 - val_accuracy: 0.7013\n","Epoch 2/10\n","200/200 [==============================] - 2s 10ms/step - loss: 0.6050 - accuracy: 0.7675 - val_loss: 0.5326 - val_accuracy: 0.8012\n","Epoch 3/10\n","200/200 [==============================] - 2s 10ms/step - loss: 0.3611 - accuracy: 0.8889 - val_loss: 0.4706 - val_accuracy: 0.8188\n","Epoch 4/10\n","200/200 [==============================] - 2s 10ms/step - loss: 0.1991 - accuracy: 0.9553 - val_loss: 0.4669 - val_accuracy: 0.8169\n","Epoch 5/10\n","200/200 [==============================] - 2s 10ms/step - loss: 0.0980 - accuracy: 0.9844 - val_loss: 0.4886 - val_accuracy: 0.8188\n","Epoch 6/10\n","200/200 [==============================] - 2s 10ms/step - loss: 0.0443 - accuracy: 0.9958 - val_loss: 0.5179 - val_accuracy: 0.8219\n","Epoch 7/10\n","200/200 [==============================] - 2s 9ms/step - loss: 0.0209 - accuracy: 0.9995 - val_loss: 0.5453 - val_accuracy: 0.8188\n","Epoch 8/10\n","200/200 [==============================] - 2s 9ms/step - loss: 0.0116 - accuracy: 0.9998 - val_loss: 0.5695 - val_accuracy: 0.8206\n","Epoch 9/10\n","200/200 [==============================] - 2s 9ms/step - loss: 0.0073 - accuracy: 0.9998 - val_loss: 0.5905 - val_accuracy: 0.8194\n","Epoch 10/10\n","200/200 [==============================] - 2s 10ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.6093 - val_accuracy: 0.8194\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TDHxW96oOFLH"},"source":["## Compare the two models"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l71IW6PSN1Sv","executionInfo":{"status":"ok","timestamp":1606656998487,"user_tz":-360,"elapsed":1086,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"3ffdc509-d40b-4265-cc42-46b362e66b7a"},"source":["print(\"Linear model on binary vectorized data:\")\n","print(binary_model.summary())"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Linear model on binary vectorized data:\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense (Dense)                (None, 4)                 40004     \n","=================================================================\n","Total params: 40,004\n","Trainable params: 40,004\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pxYKNIzBODej","executionInfo":{"status":"ok","timestamp":1606657032152,"user_tz":-360,"elapsed":902,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"2352c10e-9a5f-4c0f-b8c5-9f1d03443265"},"source":["print(\"ConvNet model on int vectorized data:\")\n","print(int_model.summary())"],"execution_count":29,"outputs":[{"output_type":"stream","text":["ConvNet model on int vectorized data:\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, None, 64)          640064    \n","_________________________________________________________________\n","conv1d (Conv1D)              (None, None, 64)          20544     \n","_________________________________________________________________\n","global_max_pooling1d (Global (None, 64)                0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 4)                 260       \n","=================================================================\n","Total params: 660,868\n","Trainable params: 660,868\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jzQujsjXOLua","executionInfo":{"status":"ok","timestamp":1606657076344,"user_tz":-360,"elapsed":5952,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"de57b116-794f-47f5-a035-3a416eb53ca6"},"source":["binary_loss, binary_accuracy = binary_model.evaluate(binary_test_ds)\n","int_loss, int_accuracy = int_model.evaluate(int_test_ds)\n","\n","print(\"Binary model accuracy: {:2.2%}\".format(binary_accuracy))\n","print(\"Int model accuracy: {:2.2%}\".format(int_accuracy))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["250/250 [==============================] - 2s 10ms/step - loss: 0.5168 - accuracy: 0.8159\n","250/250 [==============================] - 2s 10ms/step - loss: 0.6469 - accuracy: 0.8033\n","Binary model accuracy: 81.59%\n","Int model accuracy: 80.33%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q10YsGqJOjtr"},"source":["## Export the model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1sxHE4xeOVMn","executionInfo":{"status":"ok","timestamp":1606657156692,"user_tz":-360,"elapsed":4069,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"f46811e7-9ac6-4289-a091-1480bc6a25bb"},"source":["export_model = tf.keras.Sequential(\n","    [binary_vectorize_layer, binary_model,\n","     layers.Activation('sigmoid')])\n","\n","export_model.compile(\n","    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n","    optimizer='adam',\n","    metrics=['accuracy'])\n","\n","# Test it with `raw_test_ds`, which yields raw strings\n","loss, accuracy = export_model.evaluate(raw_test_ds)\n","print(\"Accuracy: {:2.2%}\".format(binary_accuracy))"],"execution_count":31,"outputs":[{"output_type":"stream","text":["250/250 [==============================] - 3s 11ms/step - loss: 0.7075 - accuracy: 0.8159\n","Accuracy: 81.59%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aDctpqUMOyo6"},"source":["***Now your model can take raw strings as input and predict a score for each label using model.predict. Define a function to find the label with the maximum score:***"]},{"cell_type":"code","metadata":{"id":"432bgHx0OpXq","executionInfo":{"status":"ok","timestamp":1606657212087,"user_tz":-360,"elapsed":944,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}}},"source":["def get_string_labels(predicted_scores_batch):\n","  predicted_int_labels = tf.argmax(predicted_scores_batch, axis=1)\n","  predicted_labels = tf.gather(raw_train_ds.class_names, predicted_int_labels)\n","  return predicted_labels"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f21TPRk5O7VQ"},"source":["## Run inference on new data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HEnscwS5O3p_","executionInfo":{"status":"ok","timestamp":1606657368051,"user_tz":-360,"elapsed":1024,"user":{"displayName":"Saim Islam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhYH9xPAYcOcpvzEzucjUC8czvhq0x0VsGIXueMzw=s64","userId":"05900108997731259678"}},"outputId":"c2663f7d-905d-4835-9328-f0dd239f92c8"},"source":["inputs = [\n","    \"how do I extract keys from a dict into a list?\",  # python\n","    \"debug public static void main(string[] args) {...}\",  # java\n","    \"panda library for data analytics\",\n","]\n","predicted_scores = export_model.predict(inputs)\n","predicted_labels = get_string_labels(predicted_scores)\n","for input, label in zip(inputs, predicted_labels):\n","  print(\"Question: \", input)\n","  print(\"Predicted label: \", label.numpy())"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Question:  how do I extract keys from a dict into a list?\n","Predicted label:  b'python'\n","Question:  debug public static void main(string[] args) {...}\n","Predicted label:  b'java'\n","Question:  panda library for data analytics\n","Predicted label:  b'python'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H-qMflBtO_e8"},"source":[""],"execution_count":null,"outputs":[]}]}